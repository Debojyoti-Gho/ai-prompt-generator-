import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
import pandas as pd
import time

# Load a comprehensive set of lightweight models
models = {
    "DistilGPT-2": "distilgpt2",
    "GPT-Neo-125M": "EleutherAI/gpt-neo-125M",
    "GPT2-Medium": "gpt2-medium",
    "OPT-125M": "facebook/opt-125m",
    "OPT-350M": "facebook/opt-350m",
    "GPT-J-6B (Lite)": "EleutherAI/gpt-j-6B",
    "FLAN-T5-Small": "google/flan-t5-small",
    "FLAN-T5-Base": "google/flan-t5-base",
}

# Initialize models and tokenizers
model_tokenizers = {
    name: AutoTokenizer.from_pretrained(model_path) for name, model_path in models.items()
}
for tokenizer in model_tokenizers.values():
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

model_objects = {
    name: AutoModelForCausalLM.from_pretrained(model_path).to("cpu") for name, model_path in models.items()
}

# Function to generate prompts using a specific model
def generate_prompt_with_model(model_name, description, modality, condition, details, max_length=100):
    base_prompt = f"Generate a {modality} scan of {description}. Focus on the medical imaging context only."
    if condition:
        base_prompt += f" Focus on capturing {condition}."
    if details:
        base_prompt += f" Include details such as {details}."
    base_prompt += " Avoid any extra information, references, or irrelevant content. Keep the prompt focused only on the image generation task."

    tokenizer = model_tokenizers[model_name]
    model = model_objects[model_name]
    inputs = tokenizer(base_prompt, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
    inputs = inputs.to("cpu")

    output = model.generate(
        inputs["input_ids"],
        max_length=max_length,
        temperature=0.7,
        do_sample=True,
        top_k=40,
        top_p=0.8,
        pad_token_id=tokenizer.pad_token_id,
    )
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated_text.strip()

# Function to evaluate the quality of the prompt
def evaluate_prompt(prompt):
    length_score = len(prompt.split())
    clarity_score = sum(1 for word in prompt.split() if len(word) > 3)
    final_score = length_score + clarity_score
    return length_score, clarity_score, final_score

# Streamlit App GUI
st.set_page_config(page_title="Multi-Model Medical Prompt Generator", layout="wide")

st.title("Multi-Model Medical Prompt Generator")
st.markdown("Generate and compare high-quality medical prompts using lightweight models.")

# Input Parameters
col1, col2 = st.columns(2)

with col1:
    description = st.text_input("Description of Image (e.g., 'human brain')", "")
    modality = st.selectbox("Imaging Modality", ["MRI", "CT", "X-ray", "Ultrasound", "PET", "SPECT"])
    condition = st.text_input("Condition to Highlight (optional)", "")

with col2:
    details = st.text_area("Additional Details (optional)", "")
    max_length = st.slider("Maximum Length of Generated Prompt", 50, 200, 100)

# Button to Generate Prompt
if st.button("Generate Prompts"):
    with st.spinner("Generating prompts from multiple models..."):
        start_time = time.time()
        try:
            results = {}
            scores = []
            comparison_data = []

            for model_name in models.keys():
                output = generate_prompt_with_model(model_name, description, modality, condition, details, max_length)
                results[model_name] = output
                length_score, clarity_score, final_score = evaluate_prompt(output)
                scores.append((model_name, length_score, clarity_score, final_score))
                comparison_data.append({
                    "Model": model_name,
                    "Prompt Length": length_score,
                    "Clarity Score": clarity_score,
                    "Final Score": final_score,
                    "Generated Prompt": output
                })

            # Determine the best model
            best_model = max(scores, key=lambda x: x[3])[0]
            best_prompt = results[best_model]

            st.success("Prompts generated successfully!")
            st.subheader(f"Best Prompt Generated by {best_model}")
            st.text_area("Best Prompt", best_prompt, height=150)

            # Comparison Metrics Table
            st.subheader("Comparison Metrics Table")
            df = pd.DataFrame(comparison_data)
            st.dataframe(df, use_container_width=True)

            # Comparison Graph
            st.subheader("Comparison Metrics Visualization")
            fig, ax = plt.subplots(figsize=(12, 8))
            model_names = [row[0] for row in scores]
            length_scores = [row[1] for row in scores]
            clarity_scores = [row[2] for row in scores]
            ax.bar(model_names, length_scores, label="Length Score", alpha=0.6)
            ax.bar(model_names, clarity_scores, label="Clarity Score", alpha=0.6, bottom=length_scores)
            ax.set_xlabel("Models")
            ax.set_ylabel("Scores")
            ax.set_title("Comparison of Model Scores")
            ax.legend()
            st.pyplot(fig)

            # Model-wise breakdown
            st.subheader("Detailed Model Outputs")
            for data in comparison_data:
                st.markdown(f"### {data['Model']}")
                st.write(f"**Prompt Length:** {data['Prompt Length']} words")
                st.write(f"**Clarity Score:** {data['Clarity Score']}")
                st.write(f"**Final Score:** {data['Final Score']}")
                st.write(f"**Generated Prompt:** {data['Generated Prompt']}")

            end_time = time.time()
            st.write(f"**Total Execution Time:** {end_time - start_time:.2f} seconds")

        except Exception as e:
            st.error(f"Error: {e}")

# Footer
st.markdown("---")
st.markdown("Â© 2024 Multi-Model Medical Prompt Generator | Powered by Open-Source Models")
